{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2895bc61-1624-458b-8914-a8de79c0227a",
   "metadata": {},
   "source": [
    "# Task 10 - RandomForest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6848e62-3c74-4850-9f20-c3acc1509fe4",
   "metadata": {},
   "source": [
    "# Dependencies and versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629c902d-7ba0-4321-a93b-4743886a97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cea50a-e45e-49a0-ac75-decb907f0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fedb555-4c6b-41ef-9934-4ad3a4c1f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e083b6-5b80-4d10-aac0-b372ef8c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24318a-91d9-4afa-846a-6afa36410474",
   "metadata": {},
   "source": [
    "# Read in the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25bbb490-05ca-47dc-924a-56d9cdce0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7545d8c-7b07-4195-99ea-b17456fbe060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_parquet = 'trends.parquet'\n",
    "# data_csv = 'trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d36ebfa6-90a6-4eb8-9196-bd6afe6f74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "229470a6-5f45-44ec-a452-0ce75c6b7f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11508d1-2477-42b7-804c-9325534160d0",
   "metadata": {},
   "source": [
    "# Convert the parquet file to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30444910-86eb-4c13-a173-e6bb295bbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ef7e25-3a33-4176-a6bd-cbadd8096456",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip:\n",
    "    sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb66834f-ac58-4bef-87b6-43047a5f800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe5f56-4d0f-4804-9e41-7f3d95e51b04",
   "metadata": {},
   "source": [
    "# Load the CSV file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ea4f30e-7302-458a-bfcc-ccbc92d7de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv',\n",
    "                              'data.csv')  # input file name \n",
    "\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3e4cdc-e7f9-4a76-b060-93d3c7afbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a49e8e-7f04-4ad0-87fc-7e9e5dd61e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', 'true').csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461efb-fe01-4a40-959b-64af5d5965dd",
   "metadata": {},
   "source": [
    "# Create a 80-20 training and test split with seed=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbfda2c9-d498-4d2f-85f4-acc1a2381d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46108e54-8019-46ee-b86c-d64f9dee0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37bae694-6a7c-4260-b134-33d1226b4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2], seed=1)\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35acda-c42a-4752-8d84-ae656e3a6949",
   "metadata": {},
   "source": [
    "# Train a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92292813-cd8f-44ab-8da5-196e58744839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.environ.get('model_target',\n",
    "                              \"model_forest.xml\")  # model output file name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cda43-6f9f-45d7-a5bb-6f1b3d138e12",
   "metadata": {},
   "source": [
    "Hyper parameters:\n",
    "\n",
    "  - number of trees : {10, 20}\n",
    "  - maximum depth : {5, 7} \n",
    "  - use random seed = 1 wherever needed\n",
    "Use the accuracy metric when evaluating the model with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19d53f63-a633-4e5e-9b8c-e3fdeb84c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac3a4aed-0722-4012-8c4a-eff5ec153328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters:(numTrees: 10 maxDepth: 5 )\n",
      "score: 0.43917802850023796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters:(numTrees: 10 maxDepth: 7 )\n",
      "score: 0.46403594725496233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters:(numTrees: 20 maxDepth: 5 )\n",
      "score: 0.44367983426187746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:=====================>                                   (3 + 5) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters:(numTrees: 20 maxDepth: 7 )\n",
      "score: 0.4664324309191187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for numTrees in [10, 20]:\n",
    "    for maxDepth in [5, 7]:\n",
    "        rfc = RandomForestClassifier(numTrees=numTrees, maxDepth=maxDepth, seed=1)\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rfc])\n",
    "\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_train)\n",
    "\n",
    "        binEval = MulticlassClassificationEvaluator(). \\\n",
    "            setMetricName(\"accuracy\"). \\\n",
    "            setPredictionCol(\"prediction\"). \\\n",
    "            setLabelCol(\"label\")\n",
    "        score = binEval.evaluate(prediction)\n",
    "\n",
    "        print(\"hyperparameters:(numTrees:\", numTrees, \"maxDepth:\", maxDepth, \")\")\n",
    "        print(\"score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2985f9-7ca9-455c-aab1-b372271335f6",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f2109-c046-4dd2-838a-cc9ec6d89851",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmmlBuilder = PMMLBuilder(sc, df_train, model)\n",
    "pmmlBuilder.buildFile(data_dir + model_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87677759-2370-449c-89ba-6148cab84d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
